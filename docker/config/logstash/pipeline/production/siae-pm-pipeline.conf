input {
  kafka {
    client_id => "siae-pm-logstash"
    group_id => "siae-pm-logstash-group"
    security_protocol => "PLAINTEXT"
    topics => ["siae-pm"]
    codec => "json"
    bootstrap_servers => ["kafka01:9092,kafka02:9092,kafka03:9092"]
  }
}

filter {
   ruby{
    code => "event.set('kafka_latency', (Time.now.getutc.to_f * 1000).to_i - event.get('ingestion_ms'))"
  }
  
  date { 
    # Set @timestamp equal to processed date
    match => ["data", "yyyy-MM-dd HH:mm:ss"]
    timezone => "UTC"
    target => "@timestamp"
  }

  mutate {
      # Also update date with processed version
      copy => { "@timestamp" => "data" }
      #Add field to distinguish already processed logs (To batch spark)
      add_field => {"spark_processed" => false}
      # Label applied by Spark ML
      add_field => {"prediction" => -1}
  } 

  mutate{
    convert => {"prediction" => "integer"}
  }
}

output {
  elasticsearch {
    hosts => ["es01:9200", "es02:9200", "es03:9200"]
    index => "siae-pm"
  }
}