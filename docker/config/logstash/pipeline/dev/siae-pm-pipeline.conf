# Logstash pipeline - DEV

input {
  # Set Kafka topic as input, JSON expected in messages
  kafka {
    client_id => "siae-pm-logstash"
    group_id => "siae-pm-logstash-group"
    topics => ["siae-pm"]
    codec => "json"
    bootstrap_servers => ["kafka:9092"]
  }
}

filter {
  # Calculate Kafka queue latency
  ruby{
    code => "event.set('kafka_latency', (Time.now.getutc.to_f * 1000).to_i - event.get('ingestion_ms'))"
  }

  # Parse date and convert to unix format
  date { 
    # Set @timestamp equal to processed date
    match => ["data", "yyyy-MM-dd HH:mm:ss"]
    timezone => "UTC"
    target => "@timestamp"
  }

  mutate {
      # Also update date with parsed version
      copy => { "@timestamp" => "data" }
      # Add field to distinguish already processed logs (To batch spark)
      add_field => {"spark_processed" => false}
      # Label applied by Spark ML
      add_field => {"prediction" => -1}
  } 

  mutate{
    convert => {"prediction" => "integer"}
  }
}

output {
  # Set elasticsearch as destination of events
  elasticsearch {
    hosts => ["es:9200"]
    index => "siae-pm"
  }
}