#
#   DATA INGESTION PIPELINE FOR MACHINE LEARNING
#   Production Version
#
#   Minimal version with one instance per application- for development purpose
#   Spark is not provided as a container - install pysapark on the local machine through pip and run 
# 
#   To run the pipeline:
#   docker compose -f docker/docker-compose-dev.yml up -d --build
#   python3 spark/ann_model/src/ann_model_batch.py 
#
#   After docker compose starts the pipeline.
#   Note: netsim must run on the same machine for kafka to work
#
#   On some host it could be required to set permission on metricbeat configuration file:
#   chmod 644 docker/config/metricbeat/dev/metricbeat.yml
#

version: "3.8"
services:
  # ELASTICSEARCH - cannot be auto scaled due to volume sharing problem - to scale, add a new service with configuration
  es:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.12.0
    environment:
      #- "ELASTICSEARCH_USERNAME=elastic"
      #- "ELASTIC_PASSWORD=password"
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      # Configuration file
      - type: bind
        source: ./config/elasticsearch/elasticsearch.yml
        target: /usr/share/elasticsearch/config/elasticsearch.yml
        read_only: true
      # Data persistence
      - type: volume
        source: es-data
        target: /usr/share/elasticsearch/data
    ports:
      - 9200:9200
      - 9300:9300
    networks:
      - cont-net

  # LOGSTASH - Can be scaled if every istance share the same pipelines
  ls:
    image: docker.elastic.co/logstash/logstash:7.12.0
    hostname: ls
    restart: always
    volumes:
      # Configuration file
      - type: bind
        source: ./config/logstash/logstash.yml
        target: /usr/share/logstash/config/logstash.yml
        read_only: true
      # Pipeline -> put each pipeline in this folder in a separate file
      - type: bind
        source: ./config/logstash/pipeline/dev
        target: /usr/share/logstash/pipeline
        read_only: true  
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks:
      - cont-net

  # KIBANA
  kib:
    image: docker.elastic.co/kibana/kibana:7.12.0
    hostname: kib
    restart: always
    volumes:
      # Configuration file
      - type: bind
        source: ./config/kibana/dev/kibana.yml
        target: /usr/share/kibana/config/kibana.yml
        read_only: true
    ports:
      - 5601:5601
    networks:
      - cont-net

  # METRICBEAT - Collect metrics about pipeline adn store it in elasticsearch
  metricbeat:
    image: docker.elastic.co/beats/metricbeat:7.12.0
    hostname: metricbeat
    volumes:
      # Configuration file
      - type: bind
        source: ./config/metricbeat/dev/metricbeat.yml
        target: /usr/share/metricbeat/metricbeat.yml
        read_only: true
      # Pipeline -> put each pipeline in this folder in a separate file
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks:
      - cont-net

  # KAFKA - Requires Zookeper to works and CMAK for gui management (optional)
  zookeeper:
    image: "bitnami/zookeeper:latest"
    hostname: zookeeper
    restart: always
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - zookeeper-data:/bitnami/zookeeper
    networks:
      - cont-net

  kafka: # if external access required expose 9092
    image: "bitnami/kafka:latest"
    hostname: kafka
    restart: always
    environment:
      - KAFKA_BROKER_ID=1
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_INTER_BROKER_LISTENER_NAME=CLIENT
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_OPTS=-javaagent:/usr/share/jolokia-jvm-1.6.1-agent.jar=port=8778,host=0.0.0.0
    volumes:
      - kafka-data:/bitnami/kafka
      - type: bind
        source: ./config/kafka/jolokia-jvm-1.6.1-agent.jar # External var needed to collect metrics
        target: /usr/share/jolokia-jvm-1.6.1-agent.jar
    ports:
      - "9093:9093"
    networks:
      - cont-net

volumes:
  es-data:
    driver: local
  zookeeper-data:
    driver: local
  kafka-data:
    driver: local

networks:
  cont-net:
    driver: bridge
